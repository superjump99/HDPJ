# 데이터 처리 자동화 파이프라인 구축

[08/12] data_parsing.py 와 data_parsing_2.py 의 코드 분석
- 데이터 처리 속도 차이가 큰 이유는 데이터 처리 방식에 근본적인 차이가 있기 때문
- 결론  
  - data_parsing.py
    - 데이터를 개별적으로 읽고 처리하는 과정에서 다수의 작은 I/O 연산과 조건 검사를 수행하기 때문에 처리 속도가 느려짐
    - 복잡한 데이터 처리 방식(조건문, 여러 번의 읽기 및 리스트 추가 작업)이 성능 저하의 주된 원인
  - data_parsing_2.py
    - 파일에서 데이터를 일괄적으로 읽고, 한 번의 struct.unpack 호출로 데이터를 해석하여 빠르게 처리
###  pcdbin_parser(input_file_path) 함수 비교

```angular2html
data_parsing.py
  1. 부분적 읽기
    - input_file의 각 데이터 필드를 개별적으로 읽음. 즉, float 형 데이터는 4바이트씩 읽고, unit16 데이터는 2바이트씩, unit32 데이터는 4바이특씩 읽음
    - 여러 번의 작은 I/O 연산이 필요하므로, 각 데이터 필드마다 읽기 작업이 여러 번 발생
  2. 조건문 사용
    - 조건문(if-elif-else)을 통해 각 필드의 데이터를 구분하여 처리
    - 각 레코드가 완성될 때까지 여러 번의 조건 검사가 이루어지며, 이 또한 처리 속도를 저하 시킴
  3. 리스트에서 데이터 분할
    - 데이터를 output_list에 임시 저장한 후 df_list에 추가
    - 이와 같은 여러 단계의 데이터 수집 및 분할 과정은 메모리와 CPU 시간을 추가적으로 소모
```
```
data_parsing_2.py
  1. 일괄 읽기
    - 각 레코드 크기(record_size)를 한 번에 읽음. 즉, INPUT_FILE.read(record_size)로 전체 레코드를 한 번에 가져와서 struct.unpack을 사용하여 각 필드를 한 번에 해석
    -  I/O 연산을 최소화하며, 한 번의 읽기 작업으로 전체 레코드를 메모리에 로드
  2. 연속적 데이터 처리
    - 한 번에 하나의 레코드를 처리하여 data 리스트에 추가합니다. 데이터 처리가 한 번에 이루어지기 때문에 처리 속도가 상대적으로 빠름
```

[08/02] 기록용: 파이썬 패키지 기능을 사용하여 각 스크립트들을 하나의 모듈로써 임포트 진행 

[06/27] 기록용: local 용량으로 인한 데이터 저장 공간 문제
- 저장공간의 이슈로 인해 dataset을 한 sequence 별로 작업할 수 있게 변경이 필요

[06/19] 기록용: pcdbin data parsing 속도 문제
- 프로젝트 초반 bin 파일을 pcd로 처리하는 부분에서 시간을 많이 소비
- 프로젝트 운영에 여유 생겨 코드 중 가장 오래걸리는 데이터 파싱 부분은 가장 먼저 개선 
- 데이터셋 파싱에 50분 정도 걸렸던게 개선된 Data_parsing_2 코드를 사용하니 한 데이터 셋에 2분 정도로 단축

## 프로젝트 소개
    - 프로젝트 이름 : LDL-OD 네트워크 학습 데이터베이스 구축
    - 프로젝트 장소 : 셀렉트스타
    - 프로젝트 기간 : 2023.12.01 ~ 2024.12.31

---

### 1. **문제 상황**
### 2. **목적**
### 3. **기대 효과**
### 4. **해결 방법**
    
### 5. **결론** 
    
### 6. **한계점**
    
### 7. **느낀점**
    

## 주요 skills
- Python
- S3
